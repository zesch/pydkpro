{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to PyDKPro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will demonstrate how to build [DKPro Core](https://dkpro.github.io/dkpro-core/) based pipelines, their processing using input strings or files, text annotation and how to use individual DKPro Core components. We also demonstrate interfacing of [spaCy](https://spacy.io/) and [nltk](http://www.nltk.org/) (python based nlp toolkits) with DKPro cas objects.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing PyDKPro\n",
    "\n",
    "PyDKPro supports Python 3.6 and above and uses [Docker](https://www.docker.com/) container which hosts web services for all Java based DKPro Core operation. To use this demo, make sure Python>=3.6 and pip package is installed. Please perform following steps (Step 1 is optional) in your terminal.\n",
    "\n",
    "1. To create vitual enviorment, \n",
    "\n",
    "    `python -m pip install virtualenv` (Install virtualenv if not already installed)\n",
    "\n",
    "    `mkdir [env_name]`\n",
    "    \n",
    "    `virtualenv -p python3 [env_name]` or `python3 -m venv [env_name]`\n",
    "    \n",
    "    To activate the environment,\n",
    "    \n",
    "    On Windows, run:\n",
    "\n",
    "    `[env_name]\\Scripts\\activate.bat`\n",
    "\n",
    "    On Unix or MacOS, run:\n",
    "\n",
    "    `source [env_name]/bin/activate`\n",
    "\n",
    "    If you want to create conda (version 4.6 or later) environment,\n",
    "\n",
    "    `conda create --name [env_name] python=3.6`\n",
    "    \n",
    "    To activate the environment (on Windows, MacOS and Unix),\n",
    "    \n",
    "    `conda activate [env_name]`\n",
    "    \n",
    "    \n",
    "\n",
    "2. Install latest python libraries: \n",
    "   \n",
    "   `python -m pip install -r requirements.txt`\n",
    "   \n",
    "   `python -m spacy download en_core_web_sm`\n",
    "   \n",
    "   \n",
    "\n",
    "3. Clone the repository.\n",
    "\n",
    "    `git clone https://github.com/zesch/pydkpro.git`\n",
    "    \n",
    "    \n",
    "\n",
    "4. Open this jupyter notebook in your browser.\n",
    "\n",
    "    `jupyter notebook`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dkpro-cassis==0.2.7 dependecies are not installed properly. Please install them and rerun this jupyter notebook\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to make sure that all dependencies are installed successfully.\n",
    "\n",
    "import pkg_resources\n",
    "from pkg_resources import DistributionNotFound, VersionConflict\n",
    "\n",
    "dependencies = [\n",
    "  'dkpro-cassis==0.2.9',\n",
    "  'spacy==2.2.1',\n",
    "  'nltk==3.4.5'\n",
    "]\n",
    " \n",
    "uninstalled_libraries = []\n",
    "for each_dependency in dependencies:\n",
    "    try:\n",
    "        pkg_resources.require([each_dependency])\n",
    "    except (DistributionNotFound, VersionConflict):\n",
    "        uninstalled_libraries.append(each_dependency)\n",
    "if len(uninstalled_libraries) > 0:\n",
    "    print(\" %s dependecies are not installed properly. Please install them and rerun this jupyter notebook\" \n",
    "          %(','.join(uninstalled_libraries)))\n",
    "else:\n",
    "    print(\"All dependencies are installed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "module_path2 = os.path.abspath(os.path.join('../pydkpro'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    sys.path.append(module_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Pipeline\n",
    "\n",
    "To process a piece of text, you'll need to first construct a `Pipeline` with different `DKPro Core` components. The pipeline is language-specific, so you'll need to first specify the language (see examples).\n",
    "\n",
    "- By default, the components of pipeline will include default parameters. However, you can always specify what parameters you want to include or change. Component parameter list is provided in [DKPro Core](https://dkpro.github.io/dkpro-core/) documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'com.zz', None)\n",
      "(b'test', None)\n",
      "(b'0.0.1-SNAPSHOT', None)\n",
      "\u001b[32m✓ Got all neccessary informations\u001b[0m\n",
      "\u001b[32m✓ Server code generation\u001b[0m\n",
      "\u001b[32m⠧\u001b[0m Compiling Project\u001b[K(b'[INFO] Scanning for projects...\\n[WARNING] \\n[WARNING] Some problems were encountered while building the effective model for com.thesis.dhoppealvarez:restapp:jar:0.0.1-SNAPSHOT\\n[WARNING] \\'build.plugins.plugin.version\\' for com.spotify:docker-maven-plugin is missing. @ line 88, column 12\\n[WARNING] \\n[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.\\n[WARNING] \\n[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.\\n[WARNING] \\n[INFO] \\n[INFO] ------------------< com.thesis.dhoppealvarez:restapp >------------------\\n[INFO] Building dkpropipeline 0.0.1-SNAPSHOT\\n[INFO] --------------------------------[ jar ]---------------------------------\\n[INFO] \\n[INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ restapp ---\\n[INFO] \\n[INFO] --- maven-resources-plugin:3.1.0:resources (default-resources) @ restapp ---\\n[INFO] Using \\'UTF-8\\' encoding to copy filtered resources.\\n[INFO] Copying 1 resource\\n[INFO] Copying 2 resources\\n[INFO] \\n[INFO] --- maven-compiler-plugin:3.8.0:compile (default-compile) @ restapp ---\\n[INFO] Changes detected - recompiling the module!\\n[INFO] Compiling 3 source files to /Users/paggarwal/github_repos/pydkpro/pydkpro/pipelines/deployment/target/classes\\n[INFO] \\n[INFO] --- maven-resources-plugin:3.1.0:testResources (default-testResources) @ restapp ---\\n[INFO] Not copying test resources\\n[INFO] \\n[INFO] --- maven-compiler-plugin:3.8.0:testCompile (default-testCompile) @ restapp ---\\n[INFO] Not compiling test sources\\n[INFO] \\n[INFO] --- maven-surefire-plugin:2.22.1:test (default-test) @ restapp ---\\n[INFO] Tests are skipped.\\n[INFO] \\n[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ restapp ---\\n[INFO] Building jar: /Users/paggarwal/github_repos/pydkpro/pydkpro/pipelines/deployment/target/restapp-0.0.1-SNAPSHOT.jar\\n[INFO] \\n[INFO] --- spring-boot-maven-plugin:2.1.0.RELEASE:repackage (repackage) @ restapp ---\\n[INFO] Replacing main artifact /Users/paggarwal/github_repos/pydkpro/pydkpro/pipelines/deployment/target/restapp-0.0.1-SNAPSHOT.jar\\n[INFO] \\n[INFO] --- docker-maven-plugin:1.2.0:build (build-image) @ restapp ---\\n[INFO] Using authentication suppliers: [ConfigFileRegistryAuthSupplier]\\n[INFO] Copying /Users/paggarwal/github_repos/pydkpro/pydkpro/pipelines/deployment/target/restapp-0.0.1-SNAPSHOT.jar -> /Users/paggarwal/github_repos/pydkpro/pydkpro/pipelines/deployment/target/docker/restapp-0.0.1-SNAPSHOT.jar\\n[INFO] Building image dkpropipeline:0.0.1-SNAPSHOT\\nStep 1/3 : FROM openjdk:8-jre-alpine\\n\\n ---> f7a292bbb70c\\nStep 2/3 : ADD /restapp-0.0.1-SNAPSHOT.jar //\\n\\n ---> 1160139fa21f\\nStep 3/3 : ENTRYPOINT [\"java\", \"-jar\", \"/restapp-0.0.1-SNAPSHOT.jar\"]\\n\\n ---> Running in c821c7838560\\nRemoving intermediate container c821c7838560\\n ---> 26ebfa85e117\\nProgressMessage{id=null, status=null, stream=null, error=null, progress=null, progressDetail=null}\\nSuccessfully built 26ebfa85e117\\nSuccessfully tagged dkpropipeline:0.0.1-SNAPSHOT\\n[INFO] Built dkpropipeline:0.0.1-SNAPSHOT\\n[INFO] \\n[INFO] --- docker-maven-plugin:1.2.0:tag (tag-image) @ restapp ---\\n[INFO] Using authentication suppliers: [ConfigFileRegistryAuthSupplier]\\n[INFO] Creating tag 134.91.18.154:5000/dkpropipeline:0.0.1-SNAPSHOT from dkpropipeline:0.0.1-SNAPSHOT\\n[INFO] \\n[INFO] --- maven-install-plugin:2.5.2:install (default-install) @ restapp ---\\n[INFO] Installing /Users/paggarwal/github_repos/pydkpro/pydkpro/pipelines/deployment/target/restapp-0.0.1-SNAPSHOT.jar to /var/root/.m2/repository/com/thesis/dhoppealvarez/restapp/0.0.1-SNAPSHOT/restapp-0.0.1-SNAPSHOT.jar\\n[INFO] Installing /Users/paggarwal/github_repos/pydkpro/pydkpro/pipelines/deployment/pom.xml to /var/root/.m2/repository/com/thesis/dhoppealvarez/restapp/0.0.1-SNAPSHOT/restapp-0.0.1-SNAPSHOT.pom\\n[INFO] ------------------------------------------------------------------------\\n[INFO] BUILD SUCCESS\\n[INFO] ------------------------------------------------------------------------\\n[INFO] Total time:  16.180 s\\n[INFO] Finished at: 2020-07-28T02:27:11+02:00\\n[INFO] ------------------------------------------------------------------------\\n', None)\n",
      "\u001b[K\u001b[32m✓\u001b[0m Compiling Project\n",
      "\u001b[K\u001b[32m✓\u001b[0m Building container...ner...\u001b[K\n",
      "\u001b[34m▐|\\____________▌\u001b[0m Container is running on port: 3000\u001b[K(b'', None)\n",
      "\u001b[KContainer web service for the provided pipeline is fired up. To stop use finish method\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pydkpro.pipeline.Pipeline at 0x10df5c908>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydkpro import Pipeline, Component  \n",
    "p = Pipeline(version=\"2.0.0\", language='en')\n",
    "p.add(Component().opennlp_segmenter())\n",
    "p.add(Component().opennlp_postagger(param_tagset=True))\n",
    "p.build() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline processing\n",
    "\n",
    "After pipeline construction, you'll need to process/trigger the pipeline with the piece of text, you want to process (see example below). If language parameter is not provided, then language detector will be used to detect the language of text. The output of processed pipeline will be `CAS` object which is container for accessing linguistic annotations having DKPro Core defined typesystem. PyDKPro provide DKPro Core type systems which are used by `CAS` object to extract the annotations e.g. `tokens`, `sentence`, `pos tags`, `ner`, etc. based on defined pipeline structure. \n",
    "\n",
    "\n",
    "The examples below demostrate how to extract token text and pos tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[32m✓\u001b[0m Pinging...\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?><xmi:XMI xmlns:xmi=\"http://www.omg.org/XMI\" xmlns:pos=\"http:///de/tudarmstadt/ukp/dkpro/core/api/lexmorph/type/pos.ecore\" xmlns:tcas=\"http:///uima/tcas.ecore\" xmlns:cas=\"http:///uima/cas.ecore\" xmlns:tweet=\"http:///de/tudarmstadt/ukp/dkpro/core/api/lexmorph/type/pos/tweet.ecore\" xmlns:morph=\"http:///de/tudarmstadt/ukp/dkpro/core/api/lexmorph/type/morph.ecore\" xmlns:type2=\"http:///de/tudarmstadt/ukp/dkpro/core/api/frequency/tfidf/type.ecore\" xmlns:dependency=\"http:///de/tudarmstadt/ukp/dkpro/core/api/syntax/type/dependency.ecore\" xmlns:type=\"http:///de/tudarmstadt/ukp/dkpro/core/api/anomaly/type.ecore\" xmlns:type6=\"http:///de/tudarmstadt/ukp/dkpro/core/api/syntax/type.ecore\" xmlns:type3=\"http:///de/tudarmstadt/ukp/dkpro/core/api/metadata/type.ecore\" xmlns:type4=\"http:///de/tudarmstadt/ukp/dkpro/core/api/ner/type.ecore\" xmlns:type5=\"http:///de/tudarmstadt/ukp/dkpro/core/api/segmentation/type.ecore\" xmlns:constituent=\"http:///de/tudarmstadt/ukp/dkpro/core/api/syntax/type/constituent.ecore\" xmlns:chunk=\"http:///de/tudarmstadt/ukp/dkpro/core/api/syntax/type/chunk.ecore\" xmi:version=\"2.0\"><cas:NULL xmi:id=\"0\"/><tcas:DocumentAnnotation xmi:id=\"8\" sofa=\"1\" begin=\"0\" end=\"20\" language=\"en\"/><type5:Sentence xmi:id=\"13\" sofa=\"1\" begin=\"0\" end=\"20\"/><type5:Token xmi:id=\"18\" sofa=\"1\" begin=\"0\" end=\"1\" pos=\"240\"/><type5:Token xmi:id=\"30\" sofa=\"1\" begin=\"1\" end=\"5\" pos=\"246\"/><type5:Token xmi:id=\"42\" sofa=\"1\" begin=\"6\" end=\"10\" pos=\"252\"/><type5:Token xmi:id=\"54\" sofa=\"1\" begin=\"11\" end=\"18\" pos=\"258\"/><type5:Token xmi:id=\"66\" sofa=\"1\" begin=\"18\" end=\"19\" pos=\"264\"/><type5:Token xmi:id=\"78\" sofa=\"1\" begin=\"19\" end=\"20\" pos=\"270\"/><type3:TagsetDescription xmi:id=\"90\" sofa=\"1\" begin=\"0\" end=\"20\" layer=\"de.tudarmstadt.ukp.dkpro.core.api.lexmorph.type.pos.POS\" name=\"ptb\" tags=\"103 105 107 109 111 113 115 117 119 121 123 125 127 129 131 133 135 137 139 141 143 145 147 149 151 153 155 157 159 161 163 165 167 169 171 173 175 177 179 181 183 185 187 189 191\" componentName=\"de.tudarmstadt.ukp.dkpro.core.opennlp.OpenNlpPosTagger\" modelLocation=\"classpath:/de/tudarmstadt/ukp/dkpro/core/opennlp/lib/tagger-en-maxent.properties\" modelVariant=\"maxent\" modelLanguage=\"en\" modelVersion=\"20120616.1\" input=\"false\"/><pos:POS_PUNCT xmi:id=\"240\" sofa=\"1\" begin=\"0\" end=\"1\" PosValue=\"``\" coarseValue=\"PUNCT\"/><pos:POS_PUNCT xmi:id=\"264\" sofa=\"1\" begin=\"18\" end=\"19\" PosValue=\".\" coarseValue=\"PUNCT\"/><pos:POS_PUNCT xmi:id=\"270\" sofa=\"1\" begin=\"19\" end=\"20\" PosValue=\"\\'\\'\" coarseValue=\"PUNCT\"/><pos:POS_NOUN xmi:id=\"246\" sofa=\"1\" begin=\"1\" end=\"5\" PosValue=\"NNS\" coarseValue=\"NOUN\"/><pos:POS_VERB xmi:id=\"252\" sofa=\"1\" begin=\"6\" end=\"10\" PosValue=\"VBP\" coarseValue=\"VERB\"/><pos:POS_PROPN xmi:id=\"258\" sofa=\"1\" begin=\"11\" end=\"18\" PosValue=\"NNP\" coarseValue=\"PROPN\"/><cas:Sofa xmi:id=\"1\" sofaNum=\"1\" sofaID=\"_InitialView\" mimeType=\"text\" sofaString=\"&quot;Lets play Cricket.&quot;\"/><type3:TagDescription xmi:id=\"103\" name=\"#\"/><type3:TagDescription xmi:id=\"105\" name=\"$\"/><type3:TagDescription xmi:id=\"107\" name=\"\\'\\'\"/><type3:TagDescription xmi:id=\"109\" name=\",\"/><type3:TagDescription xmi:id=\"111\" name=\"-LRB-\"/><type3:TagDescription xmi:id=\"113\" name=\"-RRB-\"/><type3:TagDescription xmi:id=\"115\" name=\".\"/><type3:TagDescription xmi:id=\"117\" name=\":\"/><type3:TagDescription xmi:id=\"119\" name=\"CC\"/><type3:TagDescription xmi:id=\"121\" name=\"CD\"/><type3:TagDescription xmi:id=\"123\" name=\"DT\"/><type3:TagDescription xmi:id=\"125\" name=\"EX\"/><type3:TagDescription xmi:id=\"127\" name=\"FW\"/><type3:TagDescription xmi:id=\"129\" name=\"IN\"/><type3:TagDescription xmi:id=\"131\" name=\"JJ\"/><type3:TagDescription xmi:id=\"133\" name=\"JJR\"/><type3:TagDescription xmi:id=\"135\" name=\"JJS\"/><type3:TagDescription xmi:id=\"137\" name=\"LS\"/><type3:TagDescription xmi:id=\"139\" name=\"MD\"/><type3:TagDescription xmi:id=\"141\" name=\"NN\"/><type3:TagDescription xmi:id=\"143\" name=\"NNP\"/><type3:TagDescription xmi:id=\"145\" name=\"NNPS\"/><type3:TagDescription xmi:id=\"147\" name=\"NNS\"/><type3:TagDescription xmi:id=\"149\" name=\"PDT\"/><type3:TagDescription xmi:id=\"151\" name=\"POS\"/><type3:TagDescription xmi:id=\"153\" name=\"PRP\"/><type3:TagDescription xmi:id=\"155\" name=\"PRP$\"/><type3:TagDescription xmi:id=\"157\" name=\"RB\"/><type3:TagDescription xmi:id=\"159\" name=\"RBR\"/><type3:TagDescription xmi:id=\"161\" name=\"RBS\"/><type3:TagDescription xmi:id=\"163\" name=\"RP\"/><type3:TagDescription xmi:id=\"165\" name=\"SYM\"/><type3:TagDescription xmi:id=\"167\" name=\"TO\"/><type3:TagDescription xmi:id=\"169\" name=\"UH\"/><type3:TagDescription xmi:id=\"171\" name=\"VB\"/><type3:TagDescription xmi:id=\"173\" name=\"VBD\"/><type3:TagDescription xmi:id=\"175\" name=\"VBG\"/><type3:TagDescription xmi:id=\"177\" name=\"VBN\"/><type3:TagDescription xmi:id=\"179\" name=\"VBP\"/><type3:TagDescription xmi:id=\"181\" name=\"VBZ\"/><type3:TagDescription xmi:id=\"183\" name=\"WDT\"/><type3:TagDescription xmi:id=\"185\" name=\"WP\"/><type3:TagDescription xmi:id=\"187\" name=\"WP$\"/><type3:TagDescription xmi:id=\"189\" name=\"WRB\"/><type3:TagDescription xmi:id=\"191\" name=\"``\"/><cas:View sofa=\"1\" members=\"8 13 18 30 42 54 66 78 90 240 264 270 246 252 258\"/></xmi:XMI>'\n"
     ]
    }
   ],
   "source": [
    "cas = p.process('Lets play Cricket.', language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"', 'Lets', 'play', 'Cricket', '.', '\"']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydkpro import DKProCoreTypeSystem as dts\n",
    "\n",
    "ts_token = dts().token\n",
    "\n",
    "cas.select(ts_token).as_text() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PUNCT', 'NOUN', 'VERB', 'PROPN', 'PUNCT', 'PUNCT']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas.select(ts_token).get_pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Container service is successfully destroyed'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Annotations\n",
    "\n",
    "Similar to [DKPro cassis](https://github.com/dkpro/dkpro-cassis), to add manual annotations to cas object, we need to defined it with `typesystem`. For the given text, annotations of Tokens that has an id and pos feature can be added in the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydkpro.cas import Cas\n",
    "Token = dts().typesystem.get_type('de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Token')\n",
    "cas = Cas(dts().typesystem)()\n",
    "cas.sofa_string = \"I like cheese .\"\n",
    "tokens = [\n",
    "    Token(begin=0, end=1, id='0', pos='NNP'),\n",
    "    Token(begin=2, end=6, id='1', pos='VBD'),\n",
    "    Token(begin=7, end=13, id='2', pos='IN'),\n",
    "    Token(begin=14, end=15, id='3', pos='.')\n",
    "]\n",
    "\n",
    "\n",
    "for token in tokens:\n",
    "    cas.add_annotation(token)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token features can printed as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'cheese', '.']\n",
      "['NNP', 'VBD', 'IN', '.']\n"
     ]
    }
   ],
   "source": [
    "print([x.get_covered_text() for x in cas.select_all()])\n",
    "print([x.pos for x in cas.select_all()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy interfacing\n",
    "\n",
    "Generated CAS objects can also be typecast to the spaCy annotation object model and vice-versa. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydkpro import To_spacy, From_spacy\n",
    "cas = p.process('Backgammon is one of the oldest known board games.', language='en')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion to spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backgammon NNP\n",
      "is VBZ\n",
      "one CD\n",
      "of IN\n",
      "the DT\n",
      "oldest JJS\n",
      "known VBN\n",
      "board NN\n",
      "games NNS\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "for token in To_spacy(cas)(): \n",
    "    print(token.text, token.tag_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion from spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "cas = From_spacy(doc)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNP', 'VBZ', 'NN', 'IN', 'DT', 'JJS', 'VBN', 'NN', 'NNS', '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas.select(dts().token()).get_pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK interfacing\n",
    "\n",
    "Similar to spaCy, `NLTK` objects can also be convert into cas and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydkpro.external import To_nltk, From_nltk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion to NLTK -  Since this toolkit doesn't have common dataset, PyDKPro provide helper functions e.g. `tagger` (see below example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Backgammon', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('one', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('oldest', 'JJS'),\n",
       " ('known', 'VBN'),\n",
       " ('board', 'NN'),\n",
       " ('games', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "To_nltk().tagger(cas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>}\"\"\"\n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "chunked = chunkParser.parse(To_nltk().tagger(cas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk Backgammon/NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  oldest/JJS\n",
      "  known/VBN\n",
      "  board/NN\n",
      "  games/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar helper functions are developed for NLTK to PyDKpro's CAS conversion as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "cas = From_nltk().tokenizer(TweetTokenizer().tokenize('Backgammon is one of the oldest known board games.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas processing\n",
    "\n",
    "PyDKPro pipeline also provide direct cas object processing as demonstrated in below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container web service for the provided pipeline is fired up. To stop use finish method\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pydkpro.pipeline.Pipeline at 0x1309111d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Pipeline()\n",
    "p.add(Component().stanfordPosTagger())\n",
    "p.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cas = p.process(cas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Backgammon',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'oldest',\n",
       " 'known',\n",
       " 'board',\n",
       " 'games',\n",
       " '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas.select(dts().token).as_text() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNP', 'VBZ', 'NN', 'IN', 'DT', 'JJS', 'VBN', 'NN', 'NNS', '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas.select(dts().token).get_pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcut for running single components\n",
    "\n",
    "A single component can also be run without the need to build a pipeline first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Component().clearNlpSegmenter() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Backgammon',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'oldest',\n",
       " 'known',\n",
       " 'board',\n",
       " 'games',\n",
       " '.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas = tokenizer.process('Backgammon is one of the oldest known board games.')\n",
    "cas.select(dts().token).as_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with list of strings\n",
    "\n",
    "Multiple strings in the form of list can also be processed, where each element of list will be considered as document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_list = ['Backgammon is one of the oldest known board games.', 'I like playing cricket.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games.']\n",
      "['I', 'like', 'playing', 'cricket.']\n"
     ]
    }
   ],
   "source": [
    "for str in str_list:\n",
    "    cas = p.process(str)\n",
    "    print(cas.select(dts().token).as_text()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with text documents\n",
    "\n",
    "Pipelines can also be directly run on text documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydkpro.external import File2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cas = p.process(File2str('test_data/input/test2.txt')())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Backgammon',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'oldest',\n",
       " 'known',\n",
       " 'board',\n",
       " 'games',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas.select(dts().token).as_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with multiple text documents\n",
    "\n",
    "Multiple documents can also be processed by providing documents path and document name matching patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents available at different path can be provided in list\n",
    "docs = ['test_data/input/1.txt', 'test_data/input/2.txt']\n",
    "for doc in docs:\n",
    "    p.process(File2str(doc)())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End collection process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With following command pipeline's collection process will be completed (Alternatively, scope operator `with` can be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
